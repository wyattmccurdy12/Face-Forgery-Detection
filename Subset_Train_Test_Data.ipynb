{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897ba5-e3bf-4721-ab24-7cd9946cad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98ffc3-81d2-43f7-b0f3-8e1e0963e272",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0879e7-d523-4f9d-bfd3-459f6d6b9b75",
   "metadata": {},
   "source": [
    "## 1. Create table for dataloaders (all data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc798945-ed32-40cb-8e27-1c77031e0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_csv(root_path, sub_path):\n",
    "    \"\"\"Create or load a csv file for the data loader. \n",
    "    Root path will point towards train, val, or test.\n",
    "    Sub path will point towards the image generation technique - for example fs for face-swap\n",
    "    \n",
    "    returns a nested list representing all data in csv\"\"\"\n",
    "\n",
    "    csv_path = f\"{root_path}/{sub_path}_{root_path}_table_mod1.csv\"\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Found {csv_path}. Loading data.\")\n",
    "        with open(csv_path) as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "            rows = list(csvreader)\n",
    "            return rows\n",
    "    else:\n",
    "        print(f\"Did not find csv data. Writing {csv_path}.\")\n",
    "        out_index = 0\n",
    "        n_corrupted = 0 # check to see if images load correctly\n",
    "\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            # Part of csv that is fake (we will use 0 for real, and 1 for fake)\n",
    "            fake = 1\n",
    "            for path, dirs, files in os.walk(f\"{root_path}/{sub_path}\"):\n",
    "                for name in files:\n",
    "\n",
    "                    if \".zip\" not in name: # I kept the downloaded zip files in their directory locations\n",
    "                        \n",
    "                        filepath = f\"{path}/{name}\"\n",
    "\n",
    "                        # This is part of the check for corrupted images\n",
    "                        try:\n",
    "                            im = Image.open(filepath)\n",
    "                        except:\n",
    "                            print(f\"{filepath} will not load - marked as corrupted\")\n",
    "                            n_corrupted += 1\n",
    "                            continue\n",
    "                        \n",
    "                        out_index += 1\n",
    "                        \n",
    "                        writer.writerow([out_index, filepath, fake])\n",
    "\n",
    "            fake = 0 # this is a flag\n",
    "            for path, dirs, files in os.walk(f\"{root_path}/real\"):\n",
    "                for name in files:\n",
    "                    if \".zip\" not in name:\n",
    "                        filepath = f\"{path}/{name}\"\n",
    "\n",
    "                        # check for corrupted\n",
    "                        try:\n",
    "                            im = Image.open(filepath)\n",
    "                        except:\n",
    "                            print(f\"{filepath} will not load - marked as corrupted\")\n",
    "                            n_corrupted += 1\n",
    "                            continue\n",
    "                        \n",
    "                        out_index += 1\n",
    "\n",
    "                        writer.writerow([out_index, filepath, fake])\n",
    "            \n",
    "            print(\"CSV created. Reading and loading data.\")\n",
    "            with open(csv_path) as csvfile:\n",
    "                csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "                rows = list(csvreader)\n",
    "                return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf65d4f-f7f8-424b-9549-eaee4649a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should create all of the original tables (with all data) if they do not already exist\n",
    "for root_path in [\"train\", \"test\"]:\n",
    "    for sub_path in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "        temp_rows = create_or_load_csv(root_path, sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bba2b7-3021-4f08-b5e4-93c7e674801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c30b6c-a534-46a4-b05b-188c7690ebf6",
   "metadata": {},
   "source": [
    "## 2. Create a data subset and put it in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4e89a5-8e83-4f37-a1db-db936784922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file by name\n",
    "def load_csv(root_path, sub_path):\n",
    "    csv_path = f\"{root_path}/{sub_path}_{root_path}_table_mod1.csv\"\n",
    "    with open(csv_path) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "        rows = list(csvreader)\n",
    "    return rows\n",
    "\n",
    "def write_csv(rows, outpath):\n",
    "    with open(outpath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=' ')\n",
    "        for row in rows:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "800162a8-3d05-4f8a-8306-65874a07cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do one by one\n",
    "for mod_type in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "    dsplit_type = \"train\"\n",
    "    rows = load_csv(dsplit_type, mod_type)\n",
    "    \n",
    "    real = [row for row in rows if row[2] == \"0\"]\n",
    "    fake = [row for row in rows if row[2] == \"1\"]\n",
    "\n",
    "    shuffle(real)\n",
    "    shuffle(fake) # just to make sure it is not ordered in any way\n",
    "    \n",
    "    # Subset to the amount of \n",
    "    real = real[:5_000]\n",
    "    fake = fake[:5_000]\n",
    "    \n",
    "    # concatenate the two\n",
    "    rows = real + fake\n",
    "    shuffle(rows)\n",
    "\n",
    "    for i in range(len(rows)):\n",
    "        rows[i][0] = i\n",
    "    \n",
    "    out_path = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_10k_subset.csv\"\n",
    "    write_csv(rows, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2050f0f-d96e-424a-8c8e-c72bcaa231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_type in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "    dsplit_type = \"test\"\n",
    "    rows = load_csv(dsplit_type, mod_type)\n",
    "    \n",
    "    real = [row for row in rows if row[2] == \"0\"]\n",
    "    fake = [row for row in rows if row[2] == \"1\"]\n",
    "\n",
    "    shuffle(real)\n",
    "    shuffle(fake) # just to make sure it is not ordered in any way\n",
    "    \n",
    "    # # Subset to the amount of \n",
    "    real = real[:1_000]\n",
    "    fake = fake[:1_000]\n",
    "    \n",
    "    # # concatenate the two\n",
    "    rows = real + fake\n",
    "    shuffle(rows)\n",
    "\n",
    "    for i in range(len(rows)):\n",
    "        rows[i][0] = i\n",
    "    \n",
    "    out_path = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_2k_subset.csv\"\n",
    "    write_csv(rows, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbad516-f5d5-4567-bca4-b2d75864003b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174d82be-1c67-4350-9416-2f24a9d4ed46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 'train/t2i/HPS/id_0418/image_4/1.jpg/align_sd_gene_000_000.png', '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e400b3-bf92-485e-aa04-46f414babb56",
   "metadata": {},
   "source": [
    "## 3. Transfer Images to New Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b008f018-98df-46f5-adc9-bb86e10051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e41ae1d4-829d-4108-bfca-c7f48ffa1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file by name\n",
    "def load_new_csv(root_path, sub_path):\n",
    "    csv_path = f\"subset_data/{root_path}/{sub_path}_{root_path}_2k_subset.csv\"\n",
    "    with open(csv_path) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "        rows = list(csvreader)\n",
    "    return rows\n",
    "\n",
    "def write_new_csv(rows, outpath):\n",
    "    with open(outpath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=' ')\n",
    "        for row in rows:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a66712a-70ad-4f91-b7de-28ff0aca4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outpath:  subset_data/test/t2i_test_2k_subset_update.csv\n"
     ]
    }
   ],
   "source": [
    "# Copy and rename the files from the old directory structure to the new. \n",
    "# \n",
    "\n",
    "dsplit_type = \"test\"\n",
    "mod_type = \"t2i\"\n",
    "rows = load_new_csv(dsplit_type, mod_type)\n",
    "new_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    prev_path = row[1]\n",
    "    # print(\"prev_path: \", prev_path)\n",
    "    prev_path_list = prev_path.split(\"/\")\n",
    "\n",
    "    if prev_path_list[1] == \"real\":\n",
    "        new_dir = f\"subset_data/{dsplit_type}/real\"\n",
    "    else: \n",
    "        new_dir = f\"subset_data/{dsplit_type}/{mod_type}\"\n",
    "        \n",
    "    new_path = f\"{new_dir}/{prev_path_list[-1]}\"\n",
    "\n",
    "    # new_path = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1], prev_path_list[-1]])\n",
    "    # print(\"new path: \", new_path)\n",
    "    \n",
    "    # Copy the file from the old location to the new location\n",
    "    # copy_dir = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1]])\n",
    "    # print(\"copying to: \", new_dir)\n",
    "    shutil.copy(prev_path, new_dir)\n",
    "\n",
    "#     newer_numbered_path = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1], str(i) + \".png\"])\n",
    "    newer_numbered_path = f\"{new_dir}/{mod_type}_{str(i)}.png\"\n",
    "    # print(\"newer: \", newer_numbered_path)\n",
    "    \n",
    "    os.rename(new_path, newer_numbered_path)\n",
    "    \n",
    "    row[1] = newer_numbered_path\n",
    "    # print()\n",
    "    # print()\n",
    "\n",
    "outpath = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_2k_subset_update.csv\"\n",
    "print(\"outpath: \", outpath)\n",
    "\n",
    "write_new_csv(rows, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e01d28c-4ba0-479b-9ec0-39a0c23e7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3191/3191 [00:00<00:00, 3236.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# for file in tqdm(os.listdir(\"subset_data/train/real\")):\n",
    "#     if os.path.isfile(\"subset_data/train/real/\"+file):\n",
    "#         os.remove(\"subset_data/train/real/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef1b1202-2c1b-452a-8ffd-23a96e902d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4001"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"subset_data/test/real\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5e8d6ef-321a-4032-acb7-37ada4400490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fe_7285.png'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(os.listdir(\"subset_data/train/real\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0faf5ec5-eb65-497d-8920-9f4d0000ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = random.choice(train_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e965ab93-d302-40ff-ace6-680f4533cd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train/i2i/FreeDoM_I/id_1160/save_sketch_2/3.jpg/FreeDom_Gene.png'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_path = row[1]\n",
    "prev_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7db60443-4938-4af7-91d1-cd8ca5116a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train',\n",
       " 'i2i',\n",
       " 'FreeDoM_I',\n",
       " 'id_1160',\n",
       " 'save_sketch_2',\n",
       " '3.jpg',\n",
       " 'FreeDom_Gene.png']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_path = prev_path.split(\"/\")\n",
    "prev_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0621517a-6627-4c3d-84a2-7e87c334dfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subset_data/train/i2i/1.png'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "new_path = \"/\".join([\"subset_data\", prev_path[0], prev_path[1], str(i) + \".png\"])\n",
    "new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7075966-6db1-4525-8cab-fc47fbe7ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paths = []\n",
    "for row in train_rows:\n",
    "    prev_path = row[1]\n",
    "    prev_path = prev_path.split(\"/\")\n",
    "    new_path = \"/\".join([\"subset_data\", prev_path[0], prev_path[1], prev_path[-1]])\n",
    "    new_paths.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f5a9277-db80-4a9c-9e6d-d0600583d4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "466364e4-6ce5-4f4e-bfe7-931396c5b0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5030"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(new_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b746ac1e-40cc-41a0-a848-9ac10f65d2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subset_data/train/fe/0C_interText_optDM_3_alpha=0.5.png'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_paths[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c9ac36c-8156-435a-b406-70ce5e55780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subset_data/train/fe/0C_interText_optDM_3_alpha=0.5.png'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = new_paths[8]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8320abb-b083-40b9-a091-c2e900efde4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subset_data/train/fe/_0C_interText_optDM_3_alpha=0.5.png'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.split(\"/\")\n",
    "test[-1] = \"_\" + test[-1]\n",
    "test = \"/\".join(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "92962909-8877-46fe-a5f8-63b592e6f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_no_dupe_string(s, arr):\n",
    "    while s in arr:\n",
    "        s_arr = s.split(\"/\")\n",
    "        s_arr[-1] = \"_\" + s_arr[-1]\n",
    "        s = \"/\".join(s_arr)\n",
    "    arr.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745042a5-38bb-43c1-9572-0c2defc081ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
