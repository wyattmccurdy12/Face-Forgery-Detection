{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897ba5-e3bf-4721-ab24-7cd9946cad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0879e7-d523-4f9d-bfd3-459f6d6b9b75",
   "metadata": {},
   "source": [
    "## 1. Create table for dataloaders (all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc798945-ed32-40cb-8e27-1c77031e0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was used to create original data tables, \n",
    "# but is not necessary to run in order to get the subset data - subset tables are \n",
    "# already generated\n",
    "\n",
    "# def create_or_load_csv(root_path, sub_path):\n",
    "#     \"\"\"Create or load a csv file for the data loader. \n",
    "#     Root path will point towards train, val, or test.\n",
    "#     Sub path will point towards the image generation technique - for example fs for face-swap\n",
    "    \n",
    "#     returns a nested list representing all data in csv\"\"\"\n",
    "\n",
    "#     csv_path = f\"{root_path}/{sub_path}_{root_path}_table_mod1.csv\"\n",
    "    \n",
    "#     if os.path.exists(csv_path):\n",
    "#         print(f\"Found {csv_path}. Loading data.\")\n",
    "#         with open(csv_path) as csvfile:\n",
    "#             csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "#             rows = list(csvreader)\n",
    "#             return rows\n",
    "#     else:\n",
    "#         print(f\"Did not find csv data. Writing {csv_path}.\")\n",
    "#         out_index = 0\n",
    "#         n_corrupted = 0 # check to see if images load correctly\n",
    "\n",
    "#         with open(csv_path, 'w', newline='') as csvfile:\n",
    "#             writer = csv.writer(csvfile, delimiter=' ')\n",
    "#             # Part of csv that is fake (we will use 0 for real, and 1 for fake)\n",
    "#             fake = 1\n",
    "#             for path, dirs, files in os.walk(f\"{root_path}/{sub_path}\"):\n",
    "#                 for name in files:\n",
    "\n",
    "#                     if \".zip\" not in name: # I kept the downloaded zip files in their directory locations\n",
    "                        \n",
    "#                         filepath = f\"{path}/{name}\"\n",
    "\n",
    "#                         # This is part of the check for corrupted images\n",
    "#                         try:\n",
    "#                             im = Image.open(filepath)\n",
    "#                         except:\n",
    "#                             print(f\"{filepath} will not load - marked as corrupted\")\n",
    "#                             n_corrupted += 1\n",
    "#                             continue\n",
    "                        \n",
    "#                         out_index += 1\n",
    "                        \n",
    "#                         writer.writerow([out_index, filepath, fake])\n",
    "\n",
    "#             fake = 0 # this is a flag\n",
    "#             for path, dirs, files in os.walk(f\"{root_path}/real\"):\n",
    "#                 for name in files:\n",
    "#                     if \".zip\" not in name:\n",
    "#                         filepath = f\"{path}/{name}\"\n",
    "\n",
    "#                         # check for corrupted\n",
    "#                         try:\n",
    "#                             im = Image.open(filepath)\n",
    "#                         except:\n",
    "#                             print(f\"{filepath} will not load - marked as corrupted\")\n",
    "#                             n_corrupted += 1\n",
    "#                             continue\n",
    "                        \n",
    "#                         out_index += 1\n",
    "\n",
    "#                         writer.writerow([out_index, filepath, fake])\n",
    "            \n",
    "#             print(\"CSV created. Reading and loading data.\")\n",
    "#             with open(csv_path) as csvfile:\n",
    "#                 csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "#                 rows = list(csvreader)\n",
    "#                 return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf65d4f-f7f8-424b-9549-eaee4649a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary to run if you are using subset data to replicate our paper results\n",
    "# Included for full information on \n",
    "\n",
    "# # This should create all of the original tables (with all data) if they do not already exist\n",
    "# for root_path in [\"train\", \"test\"]:\n",
    "#     for sub_path in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "#         temp_rows = create_or_load_csv(root_path, sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bba2b7-3021-4f08-b5e4-93c7e674801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c30b6c-a534-46a4-b05b-188c7690ebf6",
   "metadata": {},
   "source": [
    "## 2. Create a data subset and put it in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4e89a5-8e83-4f37-a1db-db936784922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to generate tables, \n",
    "# uncomment and run this code if you would like to create new subset tables\n",
    "\n",
    "# # Load csv file by name\n",
    "# def load_csv(root_path, sub_path):\n",
    "#     csv_path = f\"{root_path}/{sub_path}_{root_path}_table_mod1.csv\"\n",
    "#     with open(csv_path) as csvfile:\n",
    "#         csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "#         rows = list(csvreader)\n",
    "#     return rows\n",
    "\n",
    "# def write_csv(rows, outpath):\n",
    "#     with open(outpath, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile, delimiter=' ')\n",
    "#         for row in rows:\n",
    "#             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "800162a8-3d05-4f8a-8306-65874a07cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's do one by one\n",
    "# for mod_type in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "#     dsplit_type = \"train\"\n",
    "#     rows = load_csv(dsplit_type, mod_type)\n",
    "    \n",
    "#     real = [row for row in rows if row[2] == \"0\"]\n",
    "#     fake = [row for row in rows if row[2] == \"1\"]\n",
    "\n",
    "#     shuffle(real)\n",
    "#     shuffle(fake) # just to make sure it is not ordered in any way\n",
    "    \n",
    "#     # Subset to the amount of \n",
    "#     real = real[:5_000]\n",
    "#     fake = fake[:5_000]\n",
    "    \n",
    "#     # concatenate the two\n",
    "#     rows = real + fake\n",
    "#     shuffle(rows)\n",
    "\n",
    "#     for i in range(len(rows)):\n",
    "#         rows[i][0] = i\n",
    "    \n",
    "#     out_path = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_10k_subset.csv\"\n",
    "#     write_csv(rows, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2050f0f-d96e-424a-8c8e-c72bcaa231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mod_type in [\"fe\", \"fs\", \"i2i\", \"t2i\"]:\n",
    "#     dsplit_type = \"test\"\n",
    "#     rows = load_csv(dsplit_type, mod_type)\n",
    "    \n",
    "#     real = [row for row in rows if row[2] == \"0\"]\n",
    "#     fake = [row for row in rows if row[2] == \"1\"]\n",
    "\n",
    "#     shuffle(real)\n",
    "#     shuffle(fake) # just to make sure it is not ordered in any way\n",
    "    \n",
    "#     # # Subset to the amount of \n",
    "#     real = real[:1_000]\n",
    "#     fake = fake[:1_000]\n",
    "    \n",
    "#     # # concatenate the two\n",
    "#     rows = real + fake\n",
    "#     shuffle(rows)\n",
    "\n",
    "#     for i in range(len(rows)):\n",
    "#         rows[i][0] = i\n",
    "    \n",
    "#     out_path = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_2k_subset.csv\"\n",
    "#     write_csv(rows, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbad516-f5d5-4567-bca4-b2d75864003b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174d82be-1c67-4350-9416-2f24a9d4ed46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 'train/t2i/HPS/id_0418/image_4/1.jpg/align_sd_gene_000_000.png', '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e400b3-bf92-485e-aa04-46f414babb56",
   "metadata": {},
   "source": [
    "## 3. Transfer Images to New Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b008f018-98df-46f5-adc9-bb86e10051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e41ae1d4-829d-4108-bfca-c7f48ffa1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file by name\n",
    "def load_new_csv(root_path, sub_path):\n",
    "    if root_path == \"train\":\n",
    "        csv_path = f\"subset_data/{root_path}/{sub_path}_{root_path}_10k_subset.csv\"\n",
    "    elif root_path == \"test\":\n",
    "        csv_path = f\"subset_data/{root_path}/{sub_path}_{root_path}_2k_subset.csv\"\n",
    "    with open(csv_path) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\" \")\n",
    "        rows = list(csvreader)\n",
    "    return rows\n",
    "\n",
    "def write_new_csv(rows, outpath):\n",
    "    with open(outpath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=' ')\n",
    "        for row in rows:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673dd72-d144-448a-8487-7577bedc5944",
   "metadata": {},
   "source": [
    "Needed for this cell to run: \n",
    "\n",
    "    * original dataset correctly structured (see readme)\n",
    "\n",
    "    * folder for subset of data with subset csv tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a66712a-70ad-4f91-b7de-28ff0aca4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outpath:  subset_data/test/t2i_test_2k_subset_update.csv\n"
     ]
    }
   ],
   "source": [
    "# Copy and rename the files from the old directory structure to the new. \n",
    "# \n",
    "\n",
    "dsplit_type = \"test\"\n",
    "mod_type = \"t2i\"\n",
    "rows = load_new_csv(dsplit_type, mod_type)\n",
    "new_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    prev_path = row[1]\n",
    "    # print(\"prev_path: \", prev_path)\n",
    "    prev_path_list = prev_path.split(\"/\")\n",
    "\n",
    "    if prev_path_list[1] == \"real\":\n",
    "        new_dir = f\"subset_data/{dsplit_type}/real\"\n",
    "    else: \n",
    "        new_dir = f\"subset_data/{dsplit_type}/{mod_type}\"\n",
    "        \n",
    "    new_path = f\"{new_dir}/{prev_path_list[-1]}\"\n",
    "\n",
    "    # new_path = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1], prev_path_list[-1]])\n",
    "    # print(\"new path: \", new_path)\n",
    "    \n",
    "    # Copy the file from the old location to the new location\n",
    "    # copy_dir = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1]])\n",
    "    # print(\"copying to: \", new_dir)\n",
    "    shutil.copy(prev_path, new_dir)\n",
    "\n",
    "#     newer_numbered_path = \"/\".join([\"subset_data\", prev_path_list[0], prev_path_list[1], str(i) + \".png\"])\n",
    "    newer_numbered_path = f\"{new_dir}/{mod_type}_{str(i)}.png\"\n",
    "    # print(\"newer: \", newer_numbered_path)\n",
    "    \n",
    "    os.rename(new_path, newer_numbered_path)\n",
    "    \n",
    "    row[1] = newer_numbered_path\n",
    "    # print()\n",
    "    # print()\n",
    "\n",
    "outpath = f\"subset_data/{dsplit_type}/{mod_type}_{dsplit_type}_2k_subset_update.csv\"\n",
    "print(\"outpath: \", outpath)\n",
    "\n",
    "write_new_csv(rows, outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
